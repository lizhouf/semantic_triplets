# -*- coding: utf-8 -*-
"""Coreference_Triplet_for_package.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qyOZmgmMnkT4UHMsLv_3LmjkbiqPctIh

# Import Libraries
"""

# coreference
import re
import string
import pandas as pd
import networkx as nx
import itertools
from tqdm import tqdm
import spacy
nlp = spacy.load("en_core_web_lg")

# training
import re
import os
from collections import Counter
import sys
import argparse
import subprocess
import pytorch_pretrained_bert
from pytorch_pretrained_bert.modeling import BertPreTrainedModel, BertModel, BertConfig
from pytorch_pretrained_bert import BertTokenizer
from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE

import torch
from torch import nn
import torch.optim as optim
import numpy as np
import random

from torch.optim.lr_scheduler import ExponentialLR

import nltk
nltk.download('wordnet') 
from nltk.corpus import wordnet

"""# Functions and global variables for coreference

## xlsx to txt
"""

def df_cleanup_boder(filename):
    '''
    This is a function to clean up the data from Boder Corpus in an excel sheet format. 
    A cleaned txt file and a dataframe containing information about speaker, starting and ending index of sentences.
    Input:
        filename: The filename of the excel sheet. Do not include ".xlsx".
    Output:
        a dataframe containing information about speaker, starting and ending index of sentences
    '''
    df = pd.read_excel('{}.xlsx'.format(filename))
    list_texts = list(df.Texts)
    list_texts = [str(i) for i in list_texts]
    index_sent_start = []
    index_sent_end = []
    total_sent_len = 0
    speaker = []
    text_cleaned_list=[]
    text_0 = []
    for i in range(len(list_texts)):
        to_be_replaced = [" …"," -"," ,","\\'"," .",".,",",.","  ","[","]"]
        replace = [","," "," ","\\",".",",","."," ","",""]
        text = list_texts[i]
        l = len(to_be_replaced)
        for j in range(l):
            text = text.replace(to_be_replaced[j],replace[j])
        for j in range(3):
            text = text.replace(to_be_replaced[l-3],replace[l-3])  
        text = re.sub("(\d{1}),(\d{1})",r"\1\2",text)
        sentence = text.lstrip()
        text_list = [t.strip() for t in re.findall(r'\b.*?\S.*?(?:\b|$)', sentence)]
        text_line = ' '.join(text_list)
        text_line = text_line.replace(" ' t","'t") 
        text_line = text_line.replace(" ' re","'re") 
        text_line = text_line.replace(" ' s","'s") 
        text_line = text_line.replace(" ' m","'m") 
        text_line = text_line.replace("No . ","No. ") 
        text_line = text_line.replace("Mr . ","Mr. ") 
        text_line = text_line.replace("Mrs . ","Mrs. ") 
        text_line = text_line.replace("Ms . ","Ms. ") 
        text_line = text_line.replace("Dr . ","Dr. ")  
        length = len(text_line)
        if length != 0:
            text_0.append(i)
        index_sent_start.append(total_sent_len)
        total_sent_len += length
        index_sent_end.append(total_sent_len)
        total_sent_len +=1
        speaker.append(df.Speaker[i])
        text_cleaned_list.append(text_line)
    text_cleaned_list = [x for x in text_cleaned_list if len(x)>0]
    total_text = ' '.join(text_cleaned_list)
    outputfile = open("{}_cleaned.txt".format(filename), "w") 
    outputfile.write(total_text) 
    outputfile.close() 

    d = {'sentence':text_cleaned_list,'speaker':speaker, 'starting_index': index_sent_start, 'ending_index': index_sent_end}
    df_speaker_sent = pd.DataFrame(d)
    return df_speaker_sent

def df_cleanup_shoah(filename):
    '''
    This is a function to clean up the data from Shoah Corpus in an csv sheet format. 
    A cleaned txt file and a dataframe containing information about speaker, starting and ending index of sentences.
    Input:
        filename: The filename of the excel sheet. Do not include ".csv".
    Output:
        a dataframe containing information about speaker, starting and ending index of sentences
    '''
    df = pd.read_csv('{}.csv'.format(filename))
    list_texts = list(df.text)
    list_texts = [str(i) for i in list_texts]
    index_sent_start = []
    index_sent_end = []
    total_sent_len = 0
    speaker = []
    segment = []
    text_cleaned_list=[]
    text_0 = []
    for i in range(len(list_texts)):
        to_be_replaced = [" …"," -"," ,","\\'"," .",".,",",.","  "]
        replace = [","," "," ","\\",".",",","."," "]
        text = list_texts[i]
        l = len(to_be_replaced)
        for j in range(l):
            text = text.replace(to_be_replaced[j],replace[j])
        for j in range(3):
            text = text.replace(to_be_replaced[l-3],replace[l-3])  
        text = re.sub("(\d{1}),(\d{1})",r"\1\2",text)
        sentence = text.lstrip()
        text_list = [t.strip() for t in re.findall(r'\b.*?\S.*?(?:\b|$)', sentence)]
        text_line = ' '.join(text_list)
        text_line = text_line.replace(" ' t","'t") 
        text_line = text_line.replace(" ' re","'re") 
        text_line = text_line.replace(" ' s","'s") 
        text_line = text_line.replace(" ' m","'m") 
        text_line = text_line.replace("No . ","No. ") 
        text_line = text_line.replace("Mr . ","Mr. ") 
        text_line = text_line.replace("Mrs . ","Mrs. ") 
        text_line = text_line.replace("Ms . ","Ms. ") 
        text_line = text_line.replace("Dr . ","Dr. ")  
        length = len(text_line)
        if length != 0:
            text_0.append(i)
        index_sent_start.append(total_sent_len)
        total_sent_len += length
        index_sent_end.append(total_sent_len)
        total_sent_len +=1
        speaker.append(df.speaker[i])
        segment.append(df.segment[i])
        text_cleaned_list.append(text_line)

    text_cleaned_list = [x for x in text_cleaned_list if len(x)>0]
    total_text = ' '.join(text_cleaned_list)
    outputfile = open("{}_cleaned.txt".format(filename), "w") 
    outputfile.write(total_text) 
    outputfile.close() 
    d = {'sentence':text_cleaned_list,'speaker':speaker, 'segment': segment,'starting_index': index_sent_start, 'ending_index': index_sent_end}
    df_speaker_sent = pd.DataFrame(d)
    return df_speaker_sent

def df_cleanup_fortunoff(filename):
    '''
    This is a function to clean up the data from Fortunoff Corpus in an excel sheet format. 
    A cleaned txt file and a dataframe containing information about speaker, starting and ending index of sentences.
    Input:
        filename: The filename of the excel sheet. Do not include ".xlsx".
    Output:
        a dataframe containing information about speaker, starting and ending index of sentences
    '''
    df = pd.read_excel('{}.xlsx'.format(filename))
    list_texts = list(df.text)
    list_texts = [str(i) for i in list_texts]
    index_sent_start = []
    index_sent_end = []
    total_sent_len = 0
    speaker = []
    text_cleaned_list=[]
    text_0 = []
    for i in range(len(list_texts)):
        to_be_replaced = [" …"," -"," ,","\\'"," .",".,",",.","  ","[","]"]
        replace = [","," "," ","\\",".",",","."," ","",""]
        text = list_texts[i]
        l = len(to_be_replaced)
        for j in range(l):
            text = text.replace(to_be_replaced[j],replace[j])
        for j in range(3):
            text = text.replace(to_be_replaced[l-3],replace[l-3])  
        text = re.sub("(\d{1}),(\d{1})",r"\1\2",text)
        sentence = text.lstrip()
        text_list = [t.strip() for t in re.findall(r'\b.*?\S.*?(?:\b|$)', sentence)]
        text_line = ' '.join(text_list)
        text_line = text_line.replace(" ' t","'t") 
        text_line = text_line.replace(" ' re","'re") 
        text_line = text_line.replace(" ' m","'m") 
        text_line = text_line.replace(" ' s","'s") 
        text_line = text_line.replace("No . ","No. ") 
        text_line = text_line.replace("Mr . ","Mr. ") 
        text_line = text_line.replace("Mrs . ","Mrs. ") 
        text_line = text_line.replace("Ms . ","Ms. ") 
        text_line = text_line.replace("Dr . ","Dr. ") 
        length = len(text_line)
        if length != 0:
            text_0.append(i)
            index_sent_start.append(total_sent_len)
            total_sent_len += length
            index_sent_end.append(total_sent_len)
            total_sent_len +=1
            speaker.append(df.is_interviewee[i])
            text_cleaned_list.append(text_line)
    text_cleaned_list = [x for x in text_cleaned_list if len(x)>0]
    total_text = ' '.join(text_cleaned_list)
    outputfile = open("{}_cleaned.txt".format(filename), "w") 
    outputfile.write(total_text) 
    outputfile.close() 

    d = {'sentence':text_cleaned_list,'speaker':speaker, 'starting_index': index_sent_start, 'ending_index': index_sent_end}
    df_speaker_sent = pd.DataFrame(d)
    return df_speaker_sent

"""## brat_ann to training_ann"""

def brat2annconll(filename):
    '''
    This is a function to generate ann and conll files for training process based on txt and ann file in brat. 
    A transformed ann and conll file will be generated.
    Input:
        filename: The filename of the ann/txt file. Do not include ".ann" or ".txt".
    Output:
        ann file: <filename>_transformed.ann
        conll file: <filename>.conll
    '''
    # Read in original brat ann file
    annfilename = "{}.ann".format(filename)
    txtfilename = "{}.txt".format(filename) 
    file1 = open(annfilename, 'r') 
    filename = file1.name[:-4]
    Lines = file1.readlines() 

    # Split lines to two groups for MENTIONS and COREF
    count = 0
    Token = []
    Relation = []

    for line in Lines: 
        l = line.strip()
        if l[0] == 'T':
            Token.append(l.split('\t'))
        else:
            Relation.append(l.split('\t'))

    # Split Token's second term 
    for t in Token:
        t[1] = t[1].split()

    # Read txt files
    file2 = open(txtfilename, 'r') 
    Lines2 = file2.readlines() 

    # final: [Word, word_count_index, start_idx, end_idx]
    words = Lines2[0].split()
    final = []
    count = 0
    start = 0
    end=0
    for i in words:
        temp = []
        temp.append(i)
        end = start + len(i)
        temp.append(count)
        temp.append(start)
        temp.append(end)
        start = end + 1
        count += 1
        final.append(temp)
    
    # Create MENTIONS
    Mentions = []
    pron = ["i", "me","us", "we", "ourselves", "our","ours","you","your","yours","yourself","he","his","him","himself","she","her","hers","herself","they","their","theirs","themselves","it","its","itself"]
    for tok in Token:
        start_idx = 0
        end_idx = 0
        result = []
        result.append('MENTION')
        result.append(tok[0])
        for k in range(len(final)):
            if str(final[k][2]) == str(tok[1][1]):
                start_idx = k
                break
        for j in range(len(final)):
            if str(final[j][3]) == str(tok[1][2]):
                end_idx = j
                break
        result.append(0)
        result.append(start_idx)
        result.append(0)
        result.append(end_idx)
        result.append(tok[-1])
        result.append(tok[1][0])
        if len(tok[-1].split()) == 1:
            if str(tok[-1]).lower() in pron:
                result.append("PRON")
            else:
                word = tok[-1].split()
                doc = nlp(str(word))
                pos = []
                for token in doc:
                    pos.append(token.pos_)
                if "PROPN" in pos:
                    result.append("PROP")
                else:
                    result.append("NOM")    
        else:
            doc = nlp(str(tok[-1]))
            pos = []
            for token in doc:
                pos.append(token.pos_)
            if "PROPN" in pos:
                result.append("PROP")
            else:
                result.append("NOM")
        Mentions.append(result)
    
    # Extract coref pairs
    count = 0
    L = []
    for r in Relation:
        L.append((r[1].split()[1].split(':')[1], r[1].split()[2].split(':')[1]))
    
    # Grouping coref
    G=nx.from_edgelist(L)
    l=list(nx.connected_components(G))
    mapdict={z:x for x, y in enumerate(l) for z in y }
    newlist=[ x+(mapdict[x[0]],)for  x in L]
    newlist=sorted(newlist,key=lambda x : x[2])
    coref_idx_list=[list(y) for x , y in itertools.groupby(newlist,key=lambda x : x[2])]

    # Create dictionary with assigned index
    coref_dict = dict()
    for i in range(len(coref_idx_list)):
        temp = []
        for j in range(len(coref_idx_list[i])):
            temp.append(coref_idx_list[i][j][0])
            temp.append(coref_idx_list[i][j][1])
        temp = list(set(temp))
        coref_dict[i]=temp

    final_corefdict = dict()
    for key, value in coref_dict.items():
        for i in value:
            final_corefdict[i] = key

    total_idx = []
    for m in Mentions:
        total_idx.append(int(m[1][1:]))

    in_relation = []
    for i in final_corefdict.keys():
        in_relation.append(int(i[1:]))
            
    # Create COREF
    Coref = []
    for key, value in final_corefdict.items():
        temp = []
        temp.append("COREF")
        temp.append(key)
        for m in Mentions:
            if key == m[1]:
                temp.append(m[6]+'-'+str(value))
        Coref.append(temp)
    
    # Transform MENTIONS type for output convenience
    Mention = []
    for m in Mentions:
        m = [str(i) for i in m]
        Mention.append(m)

    f = open("{}_transformed.ann".format(filename), "w")
    for m in Mention:
        f.write('\t'.join(m)+"\n")
    for c in Coref:
        f.write('\t'.join(c)+"\n")
    f.close()

    # Generate conll file
    conll_lines = []
    for i in range(len(final)):
        temp = []
        temp.append(filename)
        temp.append(0)
        temp.append(final[i][1])
        temp.append(final[i][0])
        temp.append("_")
        temp.append("_")
        temp.append("_")
        temp.append("_")
        temp.append("_")
        temp.append("_")
        temp.append("_")
        temp.append("_")
        conll_lines.append(temp)

    for c in Coref:
        for m in Mentions:
            if c[1] == m[1]:
                start = m[3]
                end = m[5]
                break
        if start != end:
            conll_lines[start].append("("+str(c[2].rsplit("-", 1)[1]))
            conll_lines[end].append(str(c[2].rsplit("-", 1)[1])+")")
        else:
            conll_lines[start].append("("+str(c[2].rsplit("-")[1])+")")
    
    # Transform type for output convenience
    Conll = []
    for c in conll_lines:
        c = [str(i) for i in c]
        Conll.append(c)

    # Write and download conll file
    f = open("{}.conll".format(filename), "w")
    f.write("#begin document ({}); part 0".format(filename)+"\n")
    for c in Conll:
        f.write('\t'.join(c)+"\n")
    f.write("\n")
    f.write("#end document")
    f.close()

"""## Person List"""

def person_list():
    '''
    This is a function to create the person list for generating conll input.
    Input:
        None
    Output:
        all_list: person_list
    '''
    all_list = [['your aunt and your uncle', 24],
    ['laborers', len('laborers')],
    ['trustys', len('trustys')],
    ['the slovakian partisans', 23],
    ['my mother and my sister', 23],
    ['american jewish soldier', 23],
    ['my mother and my father', 23],
    ['anti - holocaust groups', 23],
    ['non - jewish prisoners', 22],
    ['commander', len('commander')],
    ['the weak and the sick', 21],
    ['jewish slovakian girl', 21],
    ['regular german police', 21],
    ['father with the child', 21],
    ['a non - jewish family', 21],
    ['each individual human', 21],
    ['jewish intelligentsia', 21],
    ['neighbors and friends', 21],
    ['cousins and relatives', 21],
    ['russian army', len('russian army')],
    ['american army', len('american army')],
    ['german army personnel', 21],
    ['parents and children', 20],
    ['the hungarian police', 20],
    ['brothers and sisters', 20],
    ['my father - in - law', 20],
    ['the kapos and the ss', 20],
    ['the slovak partisans', 20],
    ['american soldier', len('american soldier')],
    ['american soldiers', len('american soldiers')],
    ['one of my neighbors', 19],
    ['french soldiers', len('french soldiers')],
    ['french soldier', len('french soldier')],
    ['fellows', len('fellows')],
    ['my fellow prisoners', 19],
    ['brothers or sisters', 19],
    ['jews and non - jews', 19],
    ['a kommander fuhrer', 18],
    ['future generations', 18],
    ['jewish grandparent', 18],
    ['brother - in - law', 18],
    ['my brother ’ s son', 18],
    ['the entire family', 17],
    ['father and mother', 17],
    ['mother and father', 17],
    ['sister - in - law', 17],
    ['the grandchildren', 17],
    ['overseers', len('overseers')],
    ['pilots', len('pilots')],
    ['pilot', len('pilot')],
    ['nazi soldiers', len('nazi soldiers')],
    ['nazi soldier',len('nazi soldier')],
    ['a jewish prisoner', 17],
    ['american doctors', 16],
    ['my husband and i', 16],
    ['aunts and uncles', 16],
    ['my granddaughter', 16],
    ['the grandparents', 16],
    ['the restaurateur', 16],
    ['jewish survivors', 16],
    ['julius streicher', 16],
    ['one of our maids', 16],
    ['polish policeman', 16],
    ['russian soldiers', 16],
    ['men and children', 16],
    ['the interviewee', 15],
    ['my friend and i', 15],
    ['jewish children', 15],
    ['younger brother', 15],
    ['german official', 15],
    ['jewish students', 15],
    ['the jewish boys', 15],
    ['another doctor', 14],
    ['my little girl', 14],
    ['whole families', 14],
    ['the hungarians', 14],
    ['my fellow jews', 14],
    ['the ukrainians', 14],
    ['superintendent', 14],
    ['slave laborers', 14],
    ['rabbi', len('rabbi')],
    ['male prisoners', 14],
    ['my grandfather', 14],
    ['camp commander', 14],
    ['german people', 13],
    ['a functionary', 13],
    ['jewish parent', 13],
    ['my girlfriend', 13],
    ['jewish police', 13],
    ['young couples', 13],
    ['granddaughter', 13],
    ['jewish fellow', 13],
    ['american army', 13],
    ['the commander', 13],
    ['the conductor', 13],
    ['grandchildren', 13],
    ['block servant', 13],
    ['jewish people', 13],
    ['my stepfather', 13],
    ['whiterussians', 13],
    ['men and women', 13],
    ['gestapo chief', 13],
    ['an apprentice', 13],
    ['acquaintances', 13],
    ['jewish person', 13],
    ['the americans', 13],
    ['workers', len('workers')],
    ['the committee', 13],
    ['families', len('families')],
    ['human beings', 12],
    ['jewish child', 12],
    ['nazi teacher', 12],
    ['the attorney', 12],
    ['the director', 12],
    ['acquaintance', 12],
    ['photographer', 12],
    ['grandparents', 12],
    ['jewish girls', 12],
    ['your friend', 11],
    ['your sister', 11],
    ['supervisors', 11],
    ['dr. mengele', 11],
    ['a german ss', 11],
    ['an attorney', 11],
    ['the russian', 11],
    ['businessman', 11],
    ['human being', 11],
    ['a housewife', 11],
    ['the foremen', 11],
    ['the germans', 11],
    ['your father', 11],
    ['the peasant', 11],
    ['the masters', 11],
    ['a young man', 11],
    ['jewish boys', 11],
    ['mom and dad', 11],
    ['gentile son', 11],
    ['grandfather', 11],
    ['housekeeper', 11],
    ['aryan woman', 11],
    ['grandmother', 11],
    ['girl friend', 11],
    ['a ukrainian', 11],
    ['your mother', 11],
    ['a frenchman', 11],
    ['the gestapo', 11],
    ['gestapo man', 11],
    ['a partisan', 10],
    ['the family', 10],
    ['the greeks', 10],
    ['the french', 10],
    ['jewish men', 10],
    ['antisemite', 10],
    ['christians', 10],
    ['the police', 10],
    ['grandchild', 10],
    ['the aryans', 10],
    ['the polish', 10],
    ['yakubovich', 10],
    ['old people', 10],
    ['non - jews', 10],
    ['my parents', 10],
    ['these boys', 10],
    ['the others', 10],
    ['my brother', 10],
    ['girlfriend', 10],
    ['yourselves', 10],
    ['my husband', 10],
    ['hungarians', 10],
    ['officer', len('officer')],
    ['communists', 10],
    ['supervisor', 10],
    ['my father', 9],
    ['relatives', 9],
    ['chauffeur', 9],
    ['policeman', 9],
    ['my mother', 9],
    ['daughters', 9],
    ['customers', 9],
    ['prisoners', 9],
    ['physician', 9],
    ['my friend', 9],
    ['my sister', 9],
    ['young man', 9],
    ['wehrmacht', 9],
    ['everybody', 9],
    ['my cousin', 9],
    ['americans', 9],
    ['partisans', 9],
    ['shoemaker', 9],
    ['my people', 9],
    ['christian', 9],
    ['ourselves', 9],
    ['craftsman', 9],
    ['commandos', 9],
    ['community', 9],
    ['a peasant', 9],
    ['ss doctor', 9],
    ['the group', 9],
    ['civilians', 9],
    ['the woman', 9],
    ['daughter', 8],
    ['patients', 8],
    ['yourself', 8],
    ['gentiles', 8],
    ['brothers', 8],
    ['a german', 8],
    ['the mama', 8],
    ['teachers', 8],
    ['a family', 8],
    ['director', 8],
    ['a jewess', 8],
    ['prisoner', 8],
    ['the papa', 8],
    ['overseer', 8],
    ['the kapo', 8],
    ['everyone', 8],
    ['relative', 8],
    ['refugees', 8],
    ['soldiers', 8],
    ['my folks', 8],
    ['somebody', 8],
    ['preacher', 8],
    ['the pole', 8],
    ['children', 8],
    ['my child', 8],
    ['american', 8],
    ['students', 8],
    ['the cook', 8],
    ['russians', 8],
    ['survivor', 8],
    ['catholic', 8],
    ['peoples', 7],
    ['russian', 7],
    ['my wife', 7],
    ['someone', 7],
    ['himself', 7],
    ['corpses', 7],
    ['captain', 7],
    ['herself', 7],
    ['foreman', 7],
    ['mengele', 7],
    ['gypsies', 7],
    ['mothers', 7],
    ['husband', 7],
    ['citizen', 7],
    ['soldier', 7],
    ['midgets', 7],
    ['doctors', 7],
    ['athlete', 7],
    ['teacher', 7],
    ['gestapo', 7],
    ['germans', 7],
    ['friends', 7],
    ['grandpa', 7],
    ['anybody', 7],
    ['brother', 7],
    ['orphans', 7],
    ['peasant', 7],
    ['mangele', 7],
    ['parents', 7],
    ['my aunt', 7],
    ['priest', 6],
    ['doctor', 6],
    ['christ', 6],
    ['nobody', 6],
    ['sister', 6],
    ['people', 6],
    ['ss men', 6],
    ['worker', 6],
    ['police', 6],
    ['groups', 6],
    ['mother', 6],
    ['truman', 6],
    ['others', 6],
    ['my son', 6],
    ['friend', 6],
    ['uncles', 6],
    ['couple', 6],
    ['farmer', 6],
    ['german', 6],
    ['hitler', 6],
    ['guards', 6],
    ['nurses', 6],
    ['troops', 6],
    ['babies', 6],
    ['my man', 6],
    ['aryans', 6],
    ['driver', 6],
    ['prison', 6],
    ['father', 6],
    ['family', 6],
    ['cousin', 6],
    ['fellow', 6],
    ['parent', 6],
    ['ss man', 6],
    ['guard', 5],
    ['child', 5],
    ['aunts', 5],
    ['widow', 5],
    ['girls', 5],
    ['pimps', 5],
    ['frank', 5],
    ['nazis', 5],
    ['kapos', 5],
    ['group', 5],
    ['women', 5],
    ['poles', 5],
    ['woman', 5],
    ['aryan', 5],
    ['hoess', 5],
    ['bride', 5],
    ['capos', 5],
    ['uncle', 5],
    ['groom', 5],
    ['maids', 5],
    ['maid', 4],
    ['army', 4],
    ['nazi', 4],
    ['wife', 4],
    ['ones', 4],
    ['baby', 4],
    ['girl', 4],
    ['boss', 4],
    ['kapo', 4],
    ['them', 4],
    ['lady', 4],
    ['kids', 4],
    ['guys', 4],
    ['pole', 4],
    ['jews', 4],
    ['they', 4],
    ['aunt', 4],
    ['son', 3],
    ['jew', 3],
    ['kid', 3],
    ['one', 3],
    ['boy', 3],
    ['guy', 3],
    ['him', 3],
    ['she', 3],
    ['her', 3],
    ['god', 3],
    ['men', 3],
    ['man', 3],
    ['ss', 2],
    ['he', 2]]

"""## Prediction Input"""

def Sort(sub_li):
    '''
    This is a function to sort a list of lists based on the value of second element in a ascending order.
    Input:
        sub_li: the list that need to be sorted
    Output:
        sub_li: the sorted list based on the value of second element in a ascending order
    '''
    # reverse = None (Sorts in Ascending order)
    # key is set to sort using second element of 
    # sublist lambda has been used
    sub_li.sort(key = lambda x: x[1],reverse = True)
    return sub_li

def pred_input(pred_filename):
    '''
    This is a function to generate conll file as model prediction input based on txt file.
    Input:
        pred_filename:  The filename of the txt file. Do not include ".txt".
    Output:
        a conll file as model prediction input based on the txt file
    '''
    # Read txt file
    pred_txtfilename = "{}.txt".format(pred_filename)
    file2 = open(pred_txtfilename, 'r') 
    Lines2 = file2.readlines() 
    ex = Lines2[0]

    # Extract entity
    doc = nlp(Lines2[0])

    # find geographic names
    gpe = []
    loc = []
    fac = []
    for ent in doc.ents:
        if (ent.label_ == 'GPE'):
            gpe.append(ent.text)
        elif (ent.label_ == 'LOC'):
            loc.append(ent.text)
        elif (ent.label_ == 'FAC'):
            fac.append(ent.text)

    geographic = [x.lower() for x in list(set(gpe+loc+fac))] 

    ent = [(X.text, X.label_,X.start_char, X.end_char) for X in doc.ents if X.label_ == 'PERSON' or X.label_ == 'ORG']
    ent_new = [[X[0].lower(),len(X[0])] for X in ent if X[0].lower() not in geographic]

    # find noun chunk
    noun_chunks = [chunk.text for chunk in doc.noun_chunks]

    # find person chunk with wordnet
    syn = wordnet.synsets('god')[0] 
    spiritual_being = syn.hypernym_paths()[0][6]
    syn = wordnet.synsets('anti-semitism')[0] 
    antisemitism = syn.hypernym_paths()[0][3] 
    syn = wordnet.synsets('mother')[0] 
    person = syn.hypernym_paths()[0][3] 
    syn = wordnet.synsets('people')[0] 
    people = syn.hypernym_paths()[0][3] 
    syn = wordnet.synsets('families')[0] 
    group = syn.hypernym_paths()[0][2] 
    remove_chunk = pron = ["i", "me","us", "we", "ourselves", "our","ours","you","your","yours","yourself","his","himself","hers","herself","their","theirs","themselves","it","its","itself","who","man","one","ones","somebody","someone","person","persons"]
    all_list = []
    for chunk in noun_chunks:
        if chunk.lower() not in remove_chunk:
            split_chunk= chunk.split()
            last_word = split_chunk[-1]
            if last_word != 'school' and last_word != 'schools' and last_word != 'park':
                try:
                    syn = wordnet.synsets(last_word)[0] 
                    if (person in syn.hypernym_paths()[0]) or (people in syn.hypernym_paths()[0]) or (group in syn.hypernym_paths()[0]) or (spiritual_being in  syn.hypernym_paths()[0]):
                        all_list.append([chunk, len(chunk)])
                except:
                    pass
                    
    # black list
    black_list = []

    black_list.append('the star of david')
    black_list.append('star of david')
    # Entire person list
    all = all_list + ent_new + [['they',4],['them',4],['she',3],['her',3],['he',2],['him',3]]
    all = [[X[0].lower(),len(X[0])] for X in all if X[0].lower() not in geographic+black_list]
    all = [list(x) for x in set(tuple(x) for x in all)]
    all = Sort(all)

    after_her_person = []
    for phrase in all:
        if 'her ' == phrase[0][0:4]:
            after_her_person.append(phrase[0].split()[1:][0])

    # Find corresponding index
    new = []
    ending = []
    ex = Lines2[0]
    k = 0
    for i in range(len(ex)):
        if k == 0:
            for w in all:
                if ex[i:i+w[1]].lower() == w[0]:
                    if ex[i-1:i] == ' ' and ex[i+w[1]:i+w[1]+1] == ' ':
                        if i+w[1] not in ending:
                            new.append(list([ex[i:i+w[1]], "PERSON",i,i+w[1]]))
                            ending.append(i+w[1])
                            k = w[1]
                            break
        else:
            k -= 1

    ex_s = ex.split(' ')
    ex_word_start = []
    for i in tqdm(range(len(ex))):
        ex_s = ex.split(' ')
        ex_word_start = []
        ex_word_end = []
        prev=0
    for i in tqdm(range(len(ex_s))):
        ex_word_start.append(prev)
        cur = len(ex_s[i])
        prev += cur
        ex_word_end.append(prev)
        prev+=1

    start_idx=[]
    end_idx=[]
    
    for i in tqdm(range(len(new))):
        start = new[i][2]
        start_idx.append(ex_word_start.index(start))
        end = new[i][3]
        end_idx.append(ex_word_end.index(end))

    # Create input file
    # final: [Word, word_count_index, start_idx, end_idx]
    words = Lines2[0].split(' ')
    final = []
    count = 0
    start = 0
    end=0
    for i in tqdm(words):
        temp = []
        temp.append(i)
        end = start + len(i)
        temp.append(count)
        temp.append(start)
        temp.append(end)
        start = end + 1
        count += 1
        final.append(temp)
    
    conll_lines = []
    for i in tqdm(range(len(final))):
        temp = []
        temp.append(pred_filename)
        temp.append(0)
        temp.append(final[i][1])
        temp.append(final[i][0])
        temp.append("_")
        temp.append("_")
        temp.append("_")
        temp.append("_")
        temp.append("_")
        temp.append("_")
        temp.append("_")
        temp.append("_")
        conll_lines.append(temp)

    for i in range(len(start_idx)):
        if start_idx[i] != end_idx[i]:
            conll_lines[start_idx[i]].append("(0")
            conll_lines[end_idx[i]].append("0)")
        else:
            conll_lines[start_idx[i]].append("(0)")

    # Transform type for output convenience
    Conll = []
    for c in conll_lines:
        c = [str(i) for i in c]
        Conll.append(c)

    # Remove possessive "her"
    remove=[]
    for i in tqdm(range(len(words))):
        if words[i].lower() == 'her':
            k = words[i+1]
            doc = nlp(k)
            if [1 for token in doc if token.pos_ == "NOUN"] == [1]:
                if k not in after_her_person:
                    remove.append(final[i][1])

    for i in remove:
        if Conll[i][-1] == '(0)':
            Conll[i].pop()

    # Download
    f = open("{}.conll".format(pred_filename), "w")
    f.write("#begin document ({}); part 0".format(pred_filename)+"\n")
    for c in Conll:
        f.write('\t'.join(c)+"\n")
    f.write("\n")
    f.write("#end document")
    f.close()

pred_txtfilename = "fortunoff_1_cleaned.txt"
file2 = open(pred_txtfilename, 'r') 
Lines2 = file2.readlines() 
ex = Lines2[0]

# Extract entity
doc = nlp(Lines2[0])

# find geographic names
gpe = []
loc = []
fac = []
for ent in doc.ents:
    if (ent.label_ == 'GPE'):
        gpe.append(ent.text)
    elif (ent.label_ == 'LOC'):
        loc.append(ent.text)
    elif (ent.label_ == 'FAC'):
        fac.append(ent.text)

geographic = list(set(gpe+loc+fac))

ent = [(X.text, X.label_,X.start_char, X.end_char) for X in doc.ents if X.label_ == 'PERSON' or X.label_ == 'ORG']
ent_new = [[X[0].lower(),len(X[0])] for X in ent if X[0].lower() not in geographic]

# find noun chunk
noun_chunks = [chunk.text for chunk in doc.noun_chunks]

# find person chunk with wordnet
syn = wordnet.synsets('god')[0] 
spiritual_being = syn.hypernym_paths()[0][6]
syn = wordnet.synsets('anti-semitism')[0] 
antisemitism = syn.hypernym_paths()[0][3] 
syn = wordnet.synsets('mother')[0] 
person = syn.hypernym_paths()[0][3] 
syn = wordnet.synsets('people')[0] 
people = syn.hypernym_paths()[0][3] 
syn = wordnet.synsets('families')[0] 
group = syn.hypernym_paths()[0][2] 
remove_chunk = pron = ["i", "me","us", "we", "ourselves", "our","ours","you","your","yours","yourself","his","himself","hers","herself","their","theirs","themselves","it","its","itself","who","man","one","ones","somebody","someone","person","persons"]
all_list = []
for chunk in noun_chunks:
    if chunk.lower() not in remove_chunk:
        split_chunk= chunk.split()
        last_word = split_chunk[-1]
        if last_word != 'school' and last_word != 'schools' and last_word != 'park':
            try:
                syn = wordnet.synsets(last_word)[0] 
                if (person in syn.hypernym_paths()[0]) or (people in syn.hypernym_paths()[0]) or (group in syn.hypernym_paths()[0]) or (spiritual_being in  syn.hypernym_paths()[0]):
                    all_list.append([chunk, len(chunk)])
            except:
                pass
                    
# Entire person list
all = all_list + ent_new
all = [[X[0].lower(),len(X[0])] for X in all if X[0].lower() not in geographic]
all = [list(x) for x in set(tuple(x) for x in all)]
all = Sort(all)

after_her_person = []
for phrase in all:
    if 'her ' == phrase[0][0:4]:
        after_her_person.append(phrase[0].split()[1:][0])

doc = nlp('age')
[1 for token in doc if token.pos_ == "NOUN"]

"""## conll2brat"""

def result2brat(pred_name):
    '''
    This is a function to generate brat ann file based on model prediction preds file.
    Input:
        pred_name:  The filename of the preds file. Do not include ".preds".
    Output:
        a brat ann file based on the preds file
    '''
    pred_resultfilename = "{}.preds".format(pred_name)
    f = open(pred_resultfilename, "r")
    Lines = f.readlines() 
    lines = []
    # Split lines to two groups for MENTIONS and COREF

    for line in Lines: 
        l = line.strip()
        if len(l)!=0:
            lines.append(l)

    d = {}
    d_start = {}
    d_end = {}
    p = 0
    for i in range(len(lines)):
        if p == 0:
            if i > 0 and i < (len(lines)-1):
                line = lines[i].split('\t')
                if len(line) == 13:
                    coref_index = line[12]
                    if coref_index[0]=='(' and coref_index[len(coref_index)-1] == ')':
                        key = coref_index[1:len(coref_index)-1]
                        if key not in d:
                            d[key] = [line[3]]
                            s = 0
                            for i in range(1,i):
                                s += len(lines[i].split('\t')[3])+1
                            d_start[key] = [s]
                            s += len(lines[i+1].split('\t')[3])
                            d_end[key] = [s]
                        else:
                            d[key].append(line[3])
                            s = 0
                            for i in range(1,i):
                                s += len(lines[i].split('\t')[3])+1
                            d_start[key].append(s)
                            s += len(lines[i+1].split('\t')[3])
                            d_end[key].append(s)
                    elif coref_index[0]=='(' and coref_index[len(coref_index)-1] != ')':
                        key = coref_index[1:len(coref_index)]
                        temp = [lines[i].split('\t')[3]]
                        count = 1
                        p -= 1
                        while len(lines[i+count].split('\t')) != 13 or lines[i+count].split('\t')[12][len(lines[i+count].split('\t')[12])-1] != ')':
                            temp.append(lines[i+count].split('\t')[3])
                            count += 1
                            p -=1
                        temp.append(lines[i+count].split('\t')[3])
                        if key not in d:
                            d[key] = [" ".join(temp)]
                            s = 0
                            for i in range(1,i):
                                s += len(lines[i].split('\t')[3])+1
                            d_start[key] = [s]
                            s += len(" ".join(temp))
                            d_end[key] = [s]
                        else:
                            d[key].append(" ".join(temp))
                            s = 0
                            for i in range(1,i):
                                s += len(lines[i].split('\t')[3])+1
                            d_start[key].append(s)
                            s += len(" ".join(temp))
                            d_end[key].append(s)
        else:
            p += 1

    # Write ann file
    pred_bratannfilename = "{}.ann".format(pred_name)
    f = open(pred_bratannfilename, "w")
    c = 1
    r = 1
    for i in d.keys():
        idx = []
        for j in range(len(d[i])):
            idx.append(c)
            f.write('T'+str(c)+'\t'+'PER'+' '+ str(d_start[i][j]) + ' ' + str(d_end[i][j])+'\t'+d[i][j]+'\n')
            c+=1
        for k in range(len(idx)-1):
            f.write('R'+str(r)+'\t'+'Coreference' + ' ' + 'Arg1:' + 'T'+str(idx[k]) + ' ' + 'Arg2:' + 'T'+str(idx[k+1])+'\n')
            r += 1
    f.close()

"""# Training code (Credit to Professor David Bamman)

## bert_coref
"""

random.seed(1)
np.random.seed(1)
torch.manual_seed(1)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

bert_dim=768
HIDDEN_DIM=200

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class LSTMTagger(BertPreTrainedModel):

	def __init__(self, config, freeze_bert=False):
		super(LSTMTagger, self).__init__(config)

		hidden_dim=HIDDEN_DIM
		self.hidden_dim=hidden_dim

		self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False, do_basic_tokenize=False)
		self.bert = BertModel.from_pretrained("bert-base-cased")
		self.bert.eval()

		if freeze_bert:
			for param in self.bert.parameters():
				param.requires_grad = False

		self.distance_embeddings = nn.Embedding(11, 20)
		self.sent_distance_embeddings = nn.Embedding(11, 20)
		self.nested_embeddings = nn.Embedding(2, 20)
		self.gender_embeddings = nn.Embedding(3, 20)
		self.width_embeddings = nn.Embedding(12, 20)
		self.quote_embeddings = nn.Embedding(3, 20)

		self.lstm = nn.LSTM(4*bert_dim, hidden_dim, bidirectional=True, batch_first=True)

		self.attention1 = nn.Linear(hidden_dim * 2, hidden_dim * 2)
		self.attention2 = nn.Linear(hidden_dim * 2, 1)
		self.mention_mention1 = nn.Linear( (3 * 2 * hidden_dim + 20 + 20) * 3 + 20 + 20 + 20 + 20, 150)
		self.mention_mention2 = nn.Linear(150, 150)
		self.mention_mention3 = nn.Linear(150, 1)

		self.unary1 = nn.Linear(3 * 2 * hidden_dim + 20 + 20, 150)
		self.unary2 = nn.Linear(150, 150)
		self.unary3 = nn.Linear(150, 1)

		self.drop_layer_020 = nn.Dropout(p=0.2)
		self.tanh = nn.Tanh()

		self.apply(self.init_bert_weights)


	def get_mention_reps(self, input_ids=None, attention_mask=None, starts=None, ends=None, index=None, widths=None, quotes=None, matrix=None, transforms=None, doTrain=True):

		starts=starts.to(device)
		ends=ends.to(device)
		widths=widths.to(device)
		quotes=quotes.to(device)

		input_ids = input_ids.to(device)
		attention_mask = attention_mask.to(device)
		transforms = transforms.to(device)

		# matrix specifies which token positions (cols) are associated with which mention spans (row)
		matrix=matrix.to(device) # num_sents x max_ents x max_words

		# index specifies the location of the mentions in each sentence (which vary due to padding)
		index=index.to(device)

		sequence_outputs, pooled_outputs = self.bert(input_ids, token_type_ids=None, attention_mask=attention_mask)
		all_layers = torch.cat((sequence_outputs[-1], sequence_outputs[-2], sequence_outputs[-3], sequence_outputs[-4]), 2)
		embeds=torch.matmul(transforms,all_layers)
		lstm_output, _ = self.lstm(embeds) # num_sents x max_words x 2 * hidden_dim

		###########
		# ATTENTION OVER MENTION
		###########

		attention_weights=self.attention2(self.tanh(self.attention1(lstm_output))) # num_sents x max_words x 1
		attention_weights=torch.exp(attention_weights)
		
		attx=attention_weights.squeeze(-1).unsqueeze(1).expand_as(matrix)
		summer=attx*matrix

		val=matrix*summer # num_sents x max_ents x max_words
		
		val=val/torch.sum(1e-8+val,dim=2).unsqueeze(-1)

		attended=torch.matmul(val, lstm_output) # num_sents x max_ents x 2 * hidden_dim

		attended=attended.view(-1,2*self.hidden_dim)

		lstm_output=lstm_output.contiguous()
		position_output=lstm_output.view(-1, 2*self.hidden_dim)
		
		# starts = token position of beginning of mention in flattened token list
		start_output=torch.index_select(position_output, 0, starts)
		# ends = token position of end of mention in flattened token list
		end_output=torch.index_select(position_output, 0, ends)

		# index = index of entity in flattened list of attended mention representations
		mentions=torch.index_select(attended, 0, index)

		width_embeds=self.width_embeddings(widths)
		quote_embeds=self.quote_embeddings(quotes)

		span_representation=torch.cat((start_output, end_output, mentions, width_embeds, quote_embeds), 1)

		if doTrain:
			return span_representation
		else:
			# detach tensor from computation graph at test time or memory will blow up
			return span_representation.detach()


	def forward(self, matrix, index, truth=None, names=None, token_positions=None, starts=None, ends=None, widths=None, input_ids=None, attention_mask=None, transforms=None, quotes=None):

		doTrain=False
		if truth is not None:
			doTrain=True

		zeroTensor=torch.FloatTensor([0]).to(device)

		all_starts=None
		all_ends=None

		span_representation=None

		all_all=[]
		for b in range(len(matrix)):

			span_reps=self.get_mention_reps(input_ids=input_ids[b], attention_mask=attention_mask[b], starts=starts[b], ends=ends[b], index=index[b], widths=widths[b], quotes=quotes[b], transforms=transforms[b], matrix=matrix[b], doTrain=doTrain)
			if b == 0:
				span_representation=span_reps
				all_starts=starts[b]
				all_ends=ends[b]

			else:

				span_representation=torch.cat((span_representation, span_reps), 0)
	
				all_starts=torch.cat((all_starts, starts[b]), 0)
				all_ends=torch.cat((all_ends, ends[b]), 0)

		all_starts=all_starts.to(device)
		all_ends=all_ends.to(device)
		
		num_mentions,=all_starts.shape

		running_loss=0

		curid=-1

		curid+=1

		assignments=[]

		seen={}

		ch=0

		token_positions=np.array(token_positions)

		mention_index=np.arange(num_mentions)

		unary_scores=self.unary3(self.tanh(self.drop_layer_020(self.unary2(self.tanh(self.drop_layer_020(self.unary1(span_representation)))))))

		for i in range(num_mentions):

			if i == 0:
				# the first mention must start a new entity; this doesn't affect training (since the loss must be 0) so we can skip it.
				if truth is None:

						assignment=curid
						curid+=1
						assignments.append(assignment)
				
				continue

			MAX_PREVIOUS_MENTIONS=300

			first=0
			if truth is None:
				if len(names[i]) == 1 and names[i][0].lower() in {"he", "his", "her", "she", "him", "they", "their", "them", "it", "himself", "its", "herself", "themselves"}:
					MAX_PREVIOUS_MENTIONS=20

				first=i-MAX_PREVIOUS_MENTIONS
				if first < 0:
					first=0

			targets=span_representation[first:i]
			cp=span_representation[i].expand_as(targets)
			
			dists=[]
			nesteds=[]

			# get distance in mentions
			distances=i-mention_index[first:i]
			dists=vec_get_distance_bucket(distances)
			dists=torch.LongTensor(dists).to(device)
			distance_embeds=self.distance_embeddings(dists)

			# get distance in sentences
			sent_distances=token_positions[i]-token_positions[first:i]
			sent_dists=vec_get_distance_bucket(sent_distances)
			sent_dists=torch.LongTensor(sent_dists).to(device)
			sent_distance_embeds=self.sent_distance_embeddings(sent_dists)

			# is the current mention nested within a previous one?
			res1=(all_starts[first:i] <= all_starts[i]) 
			res2=(all_ends[i] <= all_ends[first:i])

			nesteds=(res1*res2).long()
			nesteds_embeds=self.nested_embeddings(nesteds)

			res1=(all_starts[i] <= all_starts[first:i]) 
			res2=(all_ends[first:i] <= all_ends[i])

			nesteds=(res1*res2).long()
			nesteds_embeds2=self.nested_embeddings(nesteds)

			elementwise=cp*targets
			concat=torch.cat((cp, targets, elementwise, distance_embeds, sent_distance_embeds, nesteds_embeds, nesteds_embeds2), 1)

			preds=self.mention_mention3(self.tanh(self.drop_layer_020(self.mention_mention2(self.tanh(self.drop_layer_020(self.mention_mention1(concat)))))))

			preds=preds + unary_scores[i] + unary_scores[first:i]

			preds=preds.squeeze(-1)

			if truth is not None:
	
				# zero is the score for the dummy antecedent/new entity
				preds=torch.cat((preds, zeroTensor))
	
				golds_sum=0.
				preds_sum=torch.logsumexp(preds, 0)

				if len(truth[i]) == 1 and truth[i][-1] not in seen:
					golds_sum=0.
					seen[truth[i][-1]]=1
				else:
					golds=torch.index_select(preds, 0, torch.LongTensor(truth[i]).to(device))
					golds_sum=torch.logsumexp(golds, 0)
				
				# want to maximize (golds_sum-preds_sum), so minimize (preds_sum-golds_sum)
				diff=preds_sum-golds_sum

				running_loss+=diff

			else:

				assignment=None

				if i == 0:
					assignment=curid
					curid+=1

				else:

					arg_sorts=torch.argsort(preds, descending=True)
					k=0
					while k < len(arg_sorts):
						cand_idx=arg_sorts[k]
						if preds[cand_idx] > 0:
							cand_assignment=assignments[cand_idx+first]
							assignment=cand_assignment
							ch+=1
							break

						else:
							assignment=curid
							curid+=1
							break

						k+=1


				assignments.append(assignment)

		if truth is not None:
			return running_loss
		else:
			return assignments


def get_mention_width_bucket(dist):
	if dist < 10:
		return dist + 1

	return 11

def get_distance_bucket(dist):
	if dist < 5:
		return dist+1

	elif dist >= 5 and dist <= 7:
		return 6
	elif dist >= 8 and dist <= 15:
		return 7
	elif dist >= 16 and dist <= 31:
		return 8
	elif dist >= 32 and dist <= 63:
		return 9

	return 10

vec_get_distance_bucket=np.vectorize(get_distance_bucket)

def get_inquote(start, end, sent):

	inQuote=False
	quotes=[]

	for token in sent:
		if token == "“" or token == "\"":
			if inQuote == True:
				inQuote=False
			else:
				inQuote=True

		quotes.append(inQuote)

	for i in range(start, end+1):
		if quotes[i] == True:
			return 1

	return 0


def print_conll(name, sents, all_ents, assignments, out):

	doc_id, part_id=name

	mapper=[]
	idd=0
	for ent in all_ents:
		mapper_e=[]
		for e in ent:
			mapper_e.append(idd)
			idd+=1
		mapper.append(mapper_e)

	out.write("#begin document (%s); part %s\n" % (doc_id, part_id))
	
	for s_idx, sent in enumerate(sents):
		ents=all_ents[s_idx]
		for w_idx, word in enumerate(sent):
			if w_idx == 0 or w_idx == len(sent)-1:
				continue

			label=[]
			for idx, (start, end) in enumerate(ents):
				if start == w_idx and end == w_idx:
					eid=assignments[mapper[s_idx][idx]]
					label.append("(%s)" % eid)
				elif start == w_idx:
					eid=assignments[mapper[s_idx][idx]]
					label.append("(%s" % eid)
				elif end == w_idx:
					eid=assignments[mapper[s_idx][idx]]
					label.append("%s)" % eid)

			out.write("%s\t%s\t%s\t%s\t_\t_\t_\t_\t_\t_\t_\t_\t%s\n" % (doc_id, part_id, w_idx-1, word, '|'.join(label)))

		if len(sent) > 2:
			out.write("\n")

	out.write("#end document\n")


def test(model, test_all_docs, test_all_ents, test_all_named_ents, test_all_max_words, test_all_max_ents, test_doc_names, outfile, iterr, goldFile, path_to_scorer, doTest=False):

	out=open(outfile, "w", encoding="utf-8")

	# for each document
	for idx in range(len(test_all_docs)):

		d,p=test_doc_names[idx]
		d=re.sub("/", "_", d)
		test_doc=test_all_docs[idx]
		test_ents=test_all_ents[idx]

		max_words=test_all_max_words[idx]
		max_ents=test_all_max_ents[idx]

		names=[]
		for n_idx, sent in enumerate(test_ents):
			for ent in sent:
				name=test_doc[n_idx][ent[0]:ent[1]+1]
				names.append(name)

		named_index={}
		for sidx, sentence in enumerate(test_all_named_ents[idx]):
			for start, end, _ in sentence:
				named_index[(sidx, start, end)]=1

		is_named=[]

		for sidx, sentence in enumerate(test_all_ents[idx]):
			for (start, end) in sentence:
				if (sidx, start, end) in named_index:
					is_named.append(1)
				else:
					is_named.append(0)

		test_matrix, test_index, test_token_positions, test_ent_spans, test_starts, test_ends, test_widths, test_data, test_masks, test_transforms, test_quotes=get_data(model, test_doc, test_ents, max_ents, max_words)

		assignments=model.forward(test_matrix, test_index, names=names, token_positions=test_token_positions, starts=test_starts, ends=test_ends, widths=test_widths, input_ids=test_data, attention_mask=test_masks, transforms=test_transforms, quotes=test_quotes)
		print_conll(test_doc_names[idx], test_doc, test_ents, assignments, out)

	out.close()

	if doTest:
		print("Goldfile: %s" % goldFile)
		print("Predfile: %s" % outfile)
		
		bcub_f, avg=get_conll(path_to_scorer, gold=goldFile, preds=outfile)
		print("Iter %s, Average F1: %.3f, bcub F1: %s" % (iterr, avg, bcub_f))
		sys.stdout.flush()
		return avg

def get_matrix(list_of_entities, max_words, max_ents):
	matrix=np.zeros((max_ents, max_words))
	for idx, (start, end) in enumerate(list_of_entities):
		for i in range(start, end+1):
			matrix[idx,i]=1
	return matrix


def get_data(model, doc, ents, max_ents, max_words):

	batchsize=128

	token_positions=[]
	ent_spans=[]
	persons=[]
	inquotes=[]

	batch_matrix=[]
	matrix=[]

	max_words_batch=[]
	max_ents_batch=[]

	max_w=1
	max_e=1

	sent_count=0
	for idx, sent in enumerate(doc):
		
		if len(sent) > max_w:
			max_w=len(sent)
		if len(ents[idx]) > max_e:
			max_e = len(ents[idx])

		sent_count+=1

		if sent_count == batchsize:
			max_words_batch.append(max_w)
			max_ents_batch.append(max_e)
			sent_count=0
			max_w=0
			max_e=0

	if sent_count > 0:
		max_words_batch.append(max_w)
		max_ents_batch.append(max_e)

	batch_count=0
	for idx, sent in enumerate(doc):
		matrix.append(get_matrix(ents[idx], max_words_batch[batch_count], max_ents_batch[batch_count]))

		if len(matrix) == batchsize:
			batch_matrix.append(torch.FloatTensor(matrix))
			matrix=[]
			batch_count+=1

	if len(matrix) > 0:
		batch_matrix.append(torch.FloatTensor(matrix))


	batch_index=[]
	batch_quotes=[]

	batch_ent_spans=[]

	index=[]
	abs_pos=0
	sent_count=0

	b=0
	for idx, sent in enumerate(ents):

		for i in range(len(sent)):
			index.append(sent_count*max_ents_batch[b] + i)
			s,e=sent[i]
			token_positions.append(idx)
			ent_spans.append(e-s)
			phrase=' '.join(doc[idx][s:e+1])

			inquotes.append(get_inquote(s, e, doc[idx]))


		abs_pos+=len(doc[idx])

		sent_count+=1

		if sent_count == batchsize:
			batch_index.append(torch.LongTensor(index))
			batch_quotes.append(torch.LongTensor(inquotes))
			batch_ent_spans.append(ent_spans)

			index=[]
			inquotes=[]
			ent_spans=[]
			sent_count=0
			b+=1

	if sent_count > 0:
		batch_index.append(torch.LongTensor(index))
		batch_quotes.append(torch.LongTensor(inquotes))
		batch_ent_spans.append(ent_spans)

	all_masks=[]
	all_transforms=[]
	all_data=[]

	batch_masks=[]
	batch_transforms=[]
	batch_data=[]

	# get ids and pad sentence
	for sent in doc:
		tok_ids=[]
		input_mask=[]
		transform=[]

		all_toks=[]
		n=0
		for idx, word in enumerate(sent):
			toks=model.tokenizer.tokenize(word)
			all_toks.append(toks)
			n+=len(toks)


		cur=0
		for idx, word in enumerate(sent):

			toks=all_toks[idx]
			ind=list(np.zeros(n))
			for j in range(cur,cur+len(toks)):
				ind[j]=1./len(toks)
			cur+=len(toks)
			transform.append(ind)

			tok_id=model.tokenizer.convert_tokens_to_ids(toks)
			assert len(tok_id) == len(toks)
			tok_ids.extend(tok_id)

			input_mask.extend(np.ones(len(toks)))

			token=word.lower()

		all_masks.append(input_mask)
		all_data.append(tok_ids)
		all_transforms.append(transform)

		if len(all_masks) == batchsize:
			batch_masks.append(all_masks)
			batch_data.append(all_data)
			batch_transforms.append(all_transforms)

			all_masks=[]
			all_data=[]
			all_transforms=[]

	if len(all_masks) > 0:
		batch_masks.append(all_masks)
		batch_data.append(all_data)
		batch_transforms.append(all_transforms)


	for b in range(len(batch_data)):

		max_len = max([len(sent) for sent in batch_data[b]])

		for j in range(len(batch_data[b])):
			
			blen=len(batch_data[b][j])

			for k in range(blen, max_len):
				batch_data[b][j].append(0)
				batch_masks[b][j].append(0)
				for z in range(len(batch_transforms[b][j])):
					batch_transforms[b][j][z].append(0)

			for k in range(len(batch_transforms[b][j]), max_words_batch[b]):
				batch_transforms[b][j].append(np.zeros(max_len))

		batch_data[b]=torch.LongTensor(batch_data[b])
		batch_transforms[b]=torch.FloatTensor(batch_transforms[b])
		batch_masks[b]=torch.FloatTensor(batch_masks[b])
		
	tok_pos=0
	starts=[]
	ends=[]
	widths=[]

	batch_starts=[]
	batch_ends=[]
	batch_widths=[]

	sent_count=0
	b=0
	for idx, sent in enumerate(ents):

		for i in range(len(sent)):

			s,e=sent[i]

			starts.append(tok_pos+s)
			ends.append(tok_pos+e)
			widths.append(get_mention_width_bucket(e-s))

		sent_count+=1
		tok_pos+=max_words_batch[b]

		if sent_count == batchsize:
			batch_starts.append(torch.LongTensor(starts))
			batch_ends.append(torch.LongTensor(ends))
			batch_widths.append(torch.LongTensor(widths))

			starts=[]
			ends=[]
			widths=[]
			tok_pos=0
			sent_count=0
			b+=1

	if sent_count > 0:
		batch_starts.append(torch.LongTensor(starts))
		batch_ends.append(torch.LongTensor(ends))
		batch_widths.append(torch.LongTensor(widths))


	return batch_matrix, batch_index, token_positions, ent_spans, batch_starts, batch_ends, batch_widths, batch_data, batch_masks, batch_transforms, batch_quotes


def get_ant_labels(all_doc_sents, all_doc_ents):

	max_words=0
	max_ents=0
	mention_id=0

	big_ents={}

	doc_antecedent_labels=[]
	big_doc_ents=[]

	for idx, sent in enumerate(all_doc_sents):
		if len(sent) > max_words:
			max_words=len(sent)

		this_sent_ents=[]
		all_sent_ents=sorted(all_doc_ents[idx], key=lambda x: (x[0], x[1]))
		if len(all_sent_ents) > max_ents:
			max_ents=len(all_sent_ents)

		for (w_idx_start, w_idx_end, eid) in all_sent_ents:

			this_sent_ents.append((w_idx_start, w_idx_end))

			coref={}
			if eid in big_ents:
				coref=big_ents[eid]
			else:
				coref={mention_id:1}

			vals=sorted(coref.keys())

			if eid not in big_ents:
				big_ents[eid]={}

			big_ents[eid][mention_id]=1
			mention_id+=1

			doc_antecedent_labels.append(vals)

		big_doc_ents.append(this_sent_ents)


	return doc_antecedent_labels, big_doc_ents, max_words, max_ents


def read_conll(filename, model=None):

	docid=None
	partID=None

	# collection
	all_sents=[]
	all_ents=[]
	all_antecedent_labels=[]
	all_max_words=[]
	all_max_ents=[]
	all_doc_names=[]

	all_named_ents=[]


	# for one doc
	all_doc_sents=[]
	all_doc_ents=[]
	all_doc_named_ents=[]

	# for one sentence
	sent=[]
	ents=[]
	sent.append("[SEP]")
	sid=0

	named_ents=[]
	cur_tokens=0
	max_allowable_tokens=400
	cur_tid=0
	open_count=0
	with open(filename, encoding="utf-8") as file:
		for line in file:
			if line.startswith("#begin document"):

				all_doc_ents=[]
				all_doc_sents=[]

				all_doc_named_ents=[]

				open_ents={}
				open_named_ents={}

				sid=0
				docid=None
				matcher=re.match("#begin document \((.*)\); part (.*)$", line.rstrip())
				if matcher != None:
					docid=matcher.group(1)
					partID=matcher.group(2)

				print(docid)

			elif line.startswith("#end document"):

				all_sents.append(all_doc_sents)
				
				doc_antecedent_labels, big_ents, max_words, max_ents=get_ant_labels(all_doc_sents, all_doc_ents)

				all_ents.append(big_ents)

				all_named_ents.append(all_doc_named_ents)

				all_antecedent_labels.append(doc_antecedent_labels)
				all_max_words.append(max_words+1)
				all_max_ents.append(max_ents+1)
				
				all_doc_names.append((docid,partID))

			else:

				parts=re.split("\s+", line.rstrip())
				if len(parts) < 2 or (cur_tokens >= max_allowable_tokens and open_count == 0):
		
					sent.append("[CLS]")
					all_doc_sents.append(sent)
					ents=sorted(ents, key=lambda x: (x[0], x[1]))

					all_doc_ents.append(ents)

					all_doc_named_ents.append(named_ents)

					ents=[]
					named_ents=[]
					sent=[]
					sent.append("[SEP]")
					sid+=1

					cur_tokens=0

					cur_tid=0

					if len(parts) < 2:
						continue

				# +1 to account for initial [SEP]
				tid=cur_tid+1
				token=parts[3]
				coref=parts[-1].split("|")
				b_toks=model.tokenizer.tokenize(token)
				cur_tokens+=len(b_toks)
				cur_tid+=1

				for c in coref:
					if c.startswith("(") and c.endswith(")"):
						c=re.sub("\(", "", c)
						c=int(re.sub("\)", "", c))

						ents.append((tid, tid, c))

					elif c.startswith("("):
						c=int(re.sub("\(", "", c))

						if c not in open_ents:
							open_ents[c]=[]
						open_ents[c].append(tid)
						open_count+=1

					elif c.endswith(")"):
						c=int(re.sub("\)", "", c))

						assert c in open_ents

						start_tid=open_ents[c].pop()
						open_count-=1

						ents.append((start_tid, tid, c))

				ner=parts[10].split("|")

				for c in ner:
					try:
						if c.startswith("(") and c.endswith(")"):
							c=re.sub("\(", "", c)
							c=int(re.sub("\)", "", c))

							named_ents.append((tid, tid, c))

						elif c.startswith("("):
							c=int(re.sub("\(", "", c))

							if c not in open_named_ents:
								open_named_ents[c]=[]
							open_named_ents[c].append(tid)

						elif c.endswith(")"):
							c=int(re.sub("\)", "", c))

							assert c in open_named_ents

							start_tid=open_named_ents[c].pop()

							named_ents.append((start_tid, tid, c))
					except:
						pass

				sent.append(token)

	return all_sents, all_ents, all_named_ents, all_antecedent_labels, all_max_words, all_max_ents, all_doc_names

"""## calc_coref_matrices"""

def get_coref_score(metric, path_to_scorer, gold=None, preds=None):

	output=subprocess.check_output(["perl", path_to_scorer, metric, preds, gold]).decode("utf-8")
	output=output.split("\n")[-3]
	matcher=re.search("Coreference: Recall: \(.*?\) (.*?)%	Precision: \(.*?\) (.*?)%	F1: (.*?)%", output)
	if matcher is not None:
		recall=float(matcher.group(1))
		precision=float(matcher.group(2))
		f1=float(matcher.group(3))
	return recall, precision, f1

def get_conll(path_to_scorer, gold=None, preds=None):
	bcub_r, bcub_p, bcub_f=get_coref_score("bcub", path_to_scorer, gold, preds)
	muc_r, muc_p, muc_f=get_coref_score("muc", path_to_scorer, gold, preds)
	ceaf_r, ceaf_p, ceaf_f=get_coref_score("ceafe", path_to_scorer, gold, preds)

	print("bcub:\t%.1f" % bcub_f)
	print("muc:\t%.1f" % muc_f)
	print("ceaf:\t%.1f" % ceaf_f)
	avg=(bcub_f + muc_f + ceaf_f)/3.
	print("Average F1: %.1f" % (avg))

	# Generate Latex table
	# print("%.1f&%.1f&%.1f&%.1f" % (bcub_f, muc_f, ceaf_f, avg))

	return bcub_f, avg

"""## create_crossval_train_predict_script_noneed"""

def gen(path_to_scorer, train_out_file, pred_out_file):
	train_out=open(train_out_file, "w", encoding="utf-8")
	pred_out=open(pred_out_file, "w", encoding="utf-8")

	for i in range(10):
		train_out.write ("python3 scripts/bert_coref.py -m train -w models/crossval/%s.model -t data/litbank_tenfold_splits/%s/train.conll -v data/litbank_tenfold_splits/%s/dev.conll -o preds/crossval/%s.dev.pred -s %s> logs/crossval/%s.log 2>&1\n" % (i, i, i, i, path_to_scorer, i))


		pred_out.write("python3 scripts/bert_coref.py -m predict -w models/crossval/%s.model -v data/litbank_tenfold_splits/%s/test.conll -o preds/crossval/%s.goldmentions.test.preds -s %s\n" % (i, i, i, path_to_scorer))

	train_out.close()
	pred_out.close()

"""## create_crossval"""

def create_data(ids, infolder, outfile):
	out=open(outfile, "w", encoding="utf-8")

	for idd in ids:
		infile="%s/%s.conll" % (infolder, idd)

		with open(infile) as file:
			for line in file:
				out.write("%s\n" % line.rstrip())

	out.close()

def get_ids_from_filename(filename):
	ids=[]
	with open(filename) as file:
		for line in file:
			idd=line.rstrip()
			idd=re.sub(".tsv$", "", idd)
			ids.append(idd)
	return ids

def proc(split_folder, infolder, out_top_folder,fold_num):

	for split in range(fold_num):
		train_ids=get_ids_from_filename("%s/%s/train.ids" % (split_folder, split))
		dev_ids=get_ids_from_filename("%s/%s/dev.ids" % (split_folder, split))
		test_ids=get_ids_from_filename("%s/%s/test.ids" % (split_folder, split))

		outfolder="%s/%s" % (out_top_folder, split)
		try:
			os.makedirs(outfolder)
		except:
			pass

		outTrainFile="%s/%s" % (outfolder, "train.conll")
		create_data(train_ids, infolder, outTrainFile)

		outTestFile="%s/%s" % (outfolder, "test.conll")
		create_data(test_ids, infolder, outTestFile)

		outDevFile="%s/%s" % (outfolder, "dev.conll")
		create_data(dev_ids, infolder, outDevFile)

"""# Pipeline

## Preprocessing

The original data should be in a excel sheet in a specific format as [this example](https://drive.google.com/file/d/1CUxy3A3Nku0yentuvsjHeO4Ux0tXmmb_/view?usp=sharing). We'll extract the text information from the excel sheet and save it as a txt file, and then create ann and conll files for training.
"""

# input file name as a string
filename = "Shoah_8" 
Shoah = False
Boder = False
Fortunoff = False
if filename.split('_')[0].lower() == "shoah":
    Shoah = True
if filename.split('_')[0].lower() == "boder":
    Boder = True
if filename.split('_')[0].lower() == "fortunoff":
    Fortunoff = True

# create txt file
if Boder:
    df = pd.read_excel('{}.xlsx'.format(filename))
    df_speaker_sent = df_cleanup_boder(filename)
elif Shoah:
    df = pd.read_csv('{}.csv'.format(filename))
    df_speaker_sent = df_cleanup_shoah(filename)
elif Fortunoff:
    df = pd.read_excel('{}.xlsx'.format(filename))
    df_speaker_sent = df_cleanup_fortunoff(filename)

df_speaker_sent.to_csv("{}_cleaned.csv".format(filename))

"""After having the txt file, we can start annotate our training data using brat rapid annotation tools. More detailed instructions can be found [here](https://github.com/wanxinxie/brat_coreference_project/blob/main/README.md#step1-annotation-with-brat). 

When we finish annotation, we'll have an ann file for each txt file, which we'll perform more transformation on. Before we start running the next chunk, we should import the brat ann file and brat txt file.
"""

# transform brat ann file to training ann and conll file
filename = 'Boder_117_cleaned'
brat2annconll(filename)

"""Now we have the txt, ann and conll files for training. Please follow the detailed instructions [here](https://github.com/wanxinxie/brat_coreference_project#step3-training-credit-to-ucb-professor-david-bammans-github-repo) to train the model with these files.

## Training

Create 10-fold cross-validation data
"""

# python scripts/create_crossval.py data/litbank_tenfold_splits data/original/conll/  data/litbank_tenfold_splits
fold_num = 1
filename = 'data/litbank_tenfold_splits'
infolder = 'data/original/conll/'
out_top_folder = 'data/litbank_tenfold_splits'
proc(filename, infolder, out_top_folder,fold_num)

"""Train"""

# example
trainData = 'data/litbank_tenfold_splits/0/train.conll'
modelFile= 'models/crossval/0.model' # model name
valData= 'data/litbank_tenfold_splits/0/dev.conll' # validation set
outfile= 'preds/crossval/0.dev.pred' # validation prediction
path_to_scorer= 'reference-coreference-scorers/scorer.pl'
	
cache_dir = os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(0))

model = LSTMTagger.from_pretrained('bert-base-cased',
        cache_dir=cache_dir,
        freeze_bert=True)

config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,
    num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)

model.to(device)

optimizer = optim.Adam(model.parameters(), lr=0.001)
lr_scheduler=ExponentialLR(optimizer, gamma=0.999)


all_docs, all_ents, all_named_ents, all_truth, all_max_words, all_max_ents, doc_ids=read_conll(trainData, model)
test_all_docs, test_all_ents, test_all_named_ents, test_all_truth, test_all_max_words, test_all_max_ents, test_doc_ids=read_conll(valData, model)


best_f1=0.
cur_steps=0

best_idx=0
patience=10

for i in range(100):

    model.train()
    bigloss=0.
    for idx in range(len(all_docs)):

        if idx % 10 == 0:
            print(idx, "/", len(all_docs))
            sys.stdout.flush()
        max_words=all_max_words[idx]
        max_ents=all_max_ents[idx]

        matrix, index, token_positions, ent_spans, starts, ends, widths, input_ids, masks, transforms, quotes=get_data(model, all_docs[idx], all_ents[idx], max_ents, max_words)

        if max_ents > 1:
            model.zero_grad()
            loss=model.forward(matrix, index, truth=all_truth[idx], names=None, token_positions=token_positions, starts=starts, ends=ends, widths=widths, input_ids=input_ids, attention_mask=masks, transforms=transforms, quotes=quotes)
            loss.backward()
            optimizer.step()
            cur_steps+=1
            if cur_steps % 100 == 0:
                lr_scheduler.step()
        bigloss+=loss.item()

    print(bigloss)

    model.eval()
    doTest=False
    if i >= 2:
        doTest=True
    
    avg_f1=test(model, test_all_docs, test_all_ents, test_all_named_ents, test_all_max_words, test_all_max_ents, test_doc_ids, outfile, i, valData, path_to_scorer, doTest=doTest)

    if doTest:
        if avg_f1 > best_f1:
            torch.save(model.state_dict(), modelFile)
            print("Saving model ... %.3f is better than %.3f" % (avg_f1, best_f1))
            best_f1=avg_f1
            best_idx=i

        if i-best_idx > patience:
            print ("Stopping training at epoch %s" % i)
            break

"""Create prediction"""

# example
modelFile= 'models/crossval/0.model' # model name
valData= 'data/litbank_tenfold_splits/0/test.conll' # validation set
outfile= 'preds/crossval/0.goldmentions.test.preds' # validation prediction
path_to_scorer= 'reference-coreference-scorers/scorer.pl'
	
cache_dir = os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(0))

model = LSTMTagger.from_pretrained('bert-base-cased',
        cache_dir=cache_dir,
        freeze_bert=True)

config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,
    num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)

model.to(device)

optimizer = optim.Adam(model.parameters(), lr=0.001)
lr_scheduler=ExponentialLR(optimizer, gamma=0.999)

model.load_state_dict(torch.load(modelFile, map_location=device))
model.eval()
test_all_docs, test_all_ents, test_all_named_ents, test_all_truth, test_all_max_words, test_all_max_ents, test_doc_ids=read_conll(valData, model=model)

test(model, test_all_docs, test_all_ents, test_all_named_ents, test_all_max_words, test_all_max_ents, test_doc_ids, outfile, 0, valData, path_to_scorer, doTest=True)

"""Evaluate (optional)"""

goldFile= 'data/original/conll/0.conll'
predFile= 'preds/crossval/0.goldmentions.test.preds'
scorer= 'reference-coreference-scorers/scorer.pl'
bcub_f, avg=get_conll(scorer, gold=goldFile, preds=predFile)

"""After we have the model, we can use it to predict coreference solutions. Now we need to generate the input for prediction. Make sure you import the txt file for the data that needs to be predicted on.

Generate model input
"""

pred_filename = 'fortunoff_1_cleaned'
pred_input(pred_filename)

"""Create Prediction"""

# example
modelFile= 'models/crossval/0.model' # model name
valData= 'data/litbank_tenfold_splits/0/fortunoff_1_cleaned.conll' 
outfile= 'preds/crossval/0.goldmentions.fortunoff_1_cleaned.preds' 
path_to_scorer= 'reference-coreference-scorers/scorer.pl'
	
cache_dir = os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(0))

model = LSTMTagger.from_pretrained('bert-base-cased',
        cache_dir=cache_dir,
        freeze_bert=True)

config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,
    num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)

model.to(device)

optimizer = optim.Adam(model.parameters(), lr=0.001)
lr_scheduler=ExponentialLR(optimizer, gamma=0.999)

model.load_state_dict(torch.load(modelFile, map_location=device))
model.eval()
test_all_docs, test_all_ents, test_all_named_ents, test_all_truth, test_all_max_words, test_all_max_ents, test_doc_ids=read_conll(valData, model=model)

test(model, test_all_docs, test_all_ents, test_all_named_ents, test_all_max_words, test_all_max_ents, test_doc_ids, outfile, 0, valData, path_to_scorer, doTest=True)

"""Now a corresponding conll file (eg. "prediction.conll") should be downloaded. Please follow the detailed instructions [here](https://github.com/wanxinxie/brat_coreference_project/blob/main/README.md#step4-predict-with-new-data) to generate predictions with this file.

After prediction output file is generated, we can transform the result to brat ann file if you would like to view it in brat.
"""

result2brat('fortunoff_1_cleaned') # if the result file is prediction_output.preds (make sure the result file suffix is ".preds")

"""Now you have the the brat ann file for your prediction file. Please follow the detailed instructions [here](https://github.com/wanxinxie/brat_coreference_project/blob/main/README.md#step5-view-result-in-brat) to view result in brat.


"""